---
description: Direct instructions for AI agents analyzing backend logs - tell an agent to read this
tags: [logging, diagnostics, agent-instructions, automation]
---

# Log Analysis Instructions for AI Coding Agents

**Start here if you're an AI agent asked to analyze system logs.**

---

## Your Task

Analyze backend/system logs to:
1. Identify what went wrong
2. Explain why it went wrong
3. Determine the root cause (not just symptoms)
4. Assess impact and severity
5. Recommend solutions

**Constraint**: Be systematic, not speculative. Everything you conclude must be supported by log evidence.

---

## The 7-Step Process (Use This Order)

### Step 1: Structure & Scope (Minutes 0-2)

**Do This**:
- [ ] Identify log start time and end time
- [ ] Calculate total duration
- [ ] Count log lines/entries
- [ ] Identify log severity levels present (DEBUG/INFO/WARNING/ERROR/CRITICAL)
- [ ] Look for correlation IDs or request IDs

**Output**: 
```
Timeline: 14:00:49 - 14:01:17 (28 seconds)
Total Lines: 547
Severity Distribution: 
  - DEBUG: 20%
  - INFO: 40%
  - WARNING: 15%
  - ERROR: 20%
  - CRITICAL: 5%
```

**Question to Answer**: "Do I have complete logs or are they truncated?"

---

### Step 2: Operation Tracing (Minutes 2-4)

**Do This**:
- [ ] Identify distinct user operations/requests (usually by request ID or coherent sequence)
- [ ] For each operation, trace from start to finish
- [ ] Mark each operation: âœ“ (completed successfully), âš  (partially completed), âœ— (failed)
- [ ] Extract timings: when did each operation start and end?

**Example**:
```
Operation 1: "hallo" (greeting)
  Start: [implicit from first message]
  Path: text_response
  End: [completion]
  Status: âœ“ Complete

Operation 2: "datum vandaag" (date query)
  Start: [after Op1]
  Path: text_response
  End: [completion]
  Status: âœ“ Complete

Operation 3: "status machine" (machine status)
  Start: 14:00:49
  Path: Query â†’ LLM API Call â†’ Response
  End: 14:01:17
  Status: âœ— Failed (no response delivered)
```

**Question to Answer**: "How many operations are there, which succeeded, which failed?"

---

### Step 3: Error Identification & Cataloging (Minutes 4-6)

**Do This**:
- [ ] Find ALL error/warning/critical messages
- [ ] For each, extract:
  - Message text (exact)
  - Timestamp
  - Component/module (where in code)
  - Severity level
  - Any error code or stack trace
  
- [ ] Group errors by:
  - Same error repeated? (cascading)
  - Different errors in sequence? (chained)
  - Isolated errors? (single occurrence)

**Format**:
```
ERROR #1: "WatchFiles detected changes in 'scripts/get_tree_session_data.py'"
  Time: 14:01:10
  Component: [FileWatcher/Uvicorn]
  Type: File system event
  Related: Triggered reload
  
ERROR #2: "Unexpected ASGI message 'websocket.send' after close sent"
  Time: 14:01:10
  Component: [WebSocket/ASGI]
  Type: Connection state violation
  Related: Cascade of ERROR #1
  
ERROR #3: "WebSocket already closed"
  Time: 14:01:10-14:01:17
  Component: [WebSocket]
  Type: Repeated state violation
  Related: Cascade of ERROR #2
```

**Question to Answer**: "Are these independent errors or is one causing the others (cascading)?"

---

### Step 4: Failure Point Identification (Minutes 6-8)

**Do This**:
- [ ] Find the FIRST deviation from normal/expected
- [ ] Look for a marker event that changed everything:
  - State transition error
  - Resource limit hit
  - Configuration missing
  - External dependency failure
  - Unexpected shutdown
  
- [ ] Mark the exact log line where behavior changed

**Critical Question**: "At what point did 'things going according to plan' become 'something went wrong'?"

**Example**:
```
14:00:49 [OK] Query phase starts
14:00:49 [OK] Queries execute successfully
14:00:49 [OK] LLM API call initiated
...
14:01:10 ðŸ”´ FAILURE POINT: "WatchFiles detected changes"
         â†’ Server about to reload
         â†’ LLM response still pending (27s not elapsed yet)
         â†’ WebSocket will close
         â†’ Response will be lost
14:01:10 [CONSEQUENCE] "Unexpected ASGI message 'websocket.send'"
14:01:10 [CONSEQUENCE] "WebSocket already closed" (cascades)
14:01:17 [CONSEQUENCE] "Client disconnected during processing"
```

**Output**: Exact timestamp and log line of first failure

---

### Step 5: Root Cause Analysis (Minutes 8-10)

**Do This**:
- [ ] Ask "Why?" about the failure point 3-5 times:

```
Q1: Why did "WatchFiles detected changes" happen?
A1: Because scripts/get_tree_session_data.py was modified

Q2: Why did that modification cause a shutdown?
A2: Because file watcher is configured to reload on file changes

Q3: Why did shutdown happen mid-request?
A3: Because there's no check for active connections before shutdown

Q4: Why wasn't there such a check?
A4: Because the shutdown sequence doesn't protect in-flight requests

Q5: Is that the ROOT CAUSE?
A5: YES - System lacks protection for in-flight requests during shutdown
```

- [ ] Distinguish between:
  - **Trigger**: File was modified
  - **Immediate Cause**: Reload was initiated
  - **Underlying Cause**: No protection for active requests
  - **Root Cause**: Architectural lack of request-in-flight safeguard

**Output**: Clear statement of root cause (not symptom)

**Validation**: Would fixing this prevent recurrence? YES â†’ It's root cause.

---

### Step 6: Impact & Severity Assessment (Minutes 10-12)

**Do This**:
- [ ] Determine what actually went wrong for the user
- [ ] Calculate scope:
  - Single user or all users?
  - Single operation or all operations?
  - Data lost/corrupted or just display issue?
  - Transient (one-time) or persistent?

- [ ] Assign severity:
  ```
  ðŸ”´ CRITICAL: System down, data loss, no user workaround
  ðŸŸ  HIGH: Major feature broken, significant impact
  ðŸŸ¡ MEDIUM: Feature partially broken, edge case
  ðŸŸ¢ LOW: Cosmetic or workaround available
  ```

**Example**:
```
User Experience:
  - Prompt 1: âœ“ Got response
  - Prompt 2: âœ“ Got response  
  - Prompt 3: âŒ No response (got nothing)
  
Data Impact: No data lost, but result never delivered
Scope: All users asking complex queries during file modifications
Severity: ðŸ”´ CRITICAL
  Why: Primary user interaction (machine status query) completely failed
  
User-facing effect:
  - UI likely shows "Processing..." indefinitely or timeout error
  - User got no answer to their main question
  - Backend successfully processed everything (data was retrieved, LLM responded)
  - But response never reached user
```

---

### Step 7: Expected vs Actual Behavior (Minutes 12-14)

**Do This**:
- [ ] Define what should have happened (ideal/expected path)
- [ ] Document what actually happened (from logs)
- [ ] Create comparison table

**Expected Path**:
```
1. User sends query
2. Server queries Weaviate
3. Server calls LLM API
4. LLM returns response (27-30 seconds typical)
5. Server prepares summary
6. Server sends response to frontend
7. Frontend displays response to user
8. User sees answer
```

**Actual Path**:
```
1. âœ“ User sends query
2. âœ“ Server queries Weaviate (1+5 results)
3. âœ“ Server calls LLM API
4. âœ“ LLM processes (27.75 seconds)
5. âœ“ Response ready to send
6. âŒ WebSocket closed (by file watcher shutdown)
7. âŒ Response cannot be sent
8. âŒ User sees nothing
```

**Comparison Table**:
```
| Stage | Expected | Actual | Status |
|-------|----------|--------|--------|
| Query Weaviate | Results retrieved | 1+5 results | âœ“ OK |
| Call LLM | Response in 25-30s | 27.75s response | âœ“ OK |
| LLM Processing | Completes before timeout | 200 OK received | âœ“ OK |
| Response Delivery | Sent to frontend | âŒ Socket closed | âœ— FAILED |
| User Sees | Answer in UI | No response | âœ— FAILED |
```

**Divergence Point**: Step 6 (response delivery)

---

### Step 8: Output Your Analysis (Minutes 14-15)

**Do This**:
- [ ] Create structured report (see template below)
- [ ] Include evidence (log excerpts with line numbers)
- [ ] Make recommendations in priority order
- [ ] Propose verification tests

---

## Required Output Format

Use this structure every time:

```markdown
# [System] Log Analysis - [Date/Time]

## ðŸŽ¯ Executive Summary
[1-2 sentences: What happened and why]

---

## ðŸ“Š Session Overview
- **Duration**: [start time] - [end time] = [total]
- **Operations**: [count total], âœ… [succeeded], âŒ [failed]  
- **Severity**: ðŸ”´ CRITICAL / ðŸŸ  HIGH / ðŸŸ¡ MEDIUM / ðŸŸ¢ LOW

---

## ðŸ”´ Critical Issues

### Issue #1: [Title]
- **Severity**: ðŸ”´ CRITICAL
- **Time**: [exact timestamp]
- **Message**: "[exact log message from logs]"
- **Root Cause**: [why it happened]
- **Impact**: [what went wrong for user]

### Issue #2: [Title]
[same format]

---

## ðŸ“ˆ Expected vs Actual

| Stage | Expected | Actual | Status |
|-------|----------|--------|--------|
| ... | ... | ... | ... |

---

## ðŸ” Root Cause Analysis

**The Problem**: [What specifically failed]

**Why It Happened**: 
1. Trigger: [Event that started it]
2. Cause: [Why it failed]
3. Contributing Factor: [Condition that made it worse]
4. Root Cause: [Fundamental issue]

**Evidence**:
```
[Relevant log line 1 with timestamp]
[Relevant log line 2 with timestamp]
[Relevant log line 3 with timestamp]
```

---

## ðŸ“‹ Recommendations

### ðŸ”´ CRITICAL (Highest Priority)
1. [Fix that directly addresses root cause]
2. [Second most important]

### ðŸŸ¡ IMPORTANT
3. [Improve stability/resilience]

### ðŸŸ¢ NICE-TO-HAVE
4. [Polish/optimization]

---

## âœ… Verification

To confirm fix works:
1. [Test scenario 1]
2. [Test scenario 2]
3. [Test scenario 3]

---

## ðŸ“Ž Related Documents
- Full logs: [reference]
- Related issues: [if any]
- Code changes: [recent modifications]
```

---

## Critical Rules (Don't Break These)

### âŒ DO NOT
- âŒ Speculate without evidence
- âŒ Stop at symptoms (must reach root cause)
- âŒ Mix multiple unrelated issues into one root cause
- âŒ Propose fixes before understanding cause
- âŒ Ignore cascading errors (they all stem from first error)
- âŒ Miss the "why wasn't this prevented" question

### âœ… DO
- âœ… Quote exact log messages
- âœ… Include timestamps
- âœ… Distinguish cause from symptom
- âœ… Ask "Why?" multiple times
- âœ… Validate root cause with evidence
- âœ… Create expected vs actual comparison
- âœ… Prioritize recommendations by impact
- âœ… Propose testable verification steps

---

## Key Questions You Must Answer

1. â“ **What**: In one sentence, what failed?
2. â“ **Why**: In one sentence, why did it fail (root cause)?
3. â“ **When**: Exact timestamp of first failure
4. â“ **Component**: Which system/module?
5. â“ **Impact**: What did the user experience?
6. â“ **Prevention**: What safeguard was missing?
7. â“ **Fix**: What are top 3 changes?
8. â“ **Verify**: How do you test the fix?

**If you can't answer all 8, keep analyzing.**

---

## Debugging Heuristics

### When You See Error Cascade
```
Error A at 14:01:10
  â””â”€ Error B at 14:01:10 (same timestamp = B caused by A)
     â””â”€ Error C at 14:01:10 (same timestamp = C caused by B)
        â””â”€ Error D at 14:01:11 (different timestamp = D independent or delayed)

Action: Fix the root (Error A), others disappear
```

### When You See Multiple Operations
```
Op1 logs âœ“ â†’ Op2 logs âœ“ â†’ Op3 logs âœ“ â†’ ERROR â†’ No more Op3 logs

Meaning: Error happened during Op3
Action: Focus analysis on Op3 logs
```

### When Everything Looks Normal Until It's Not
```
[Normal logs for 20 lines]
[Normal logs for 10 lines]
ðŸ”´ Sudden ERROR at line 42
[Error cascade: 5+ related errors]

Action: Line 42 is the failure point, work backward to find why
```

### When You See Timing Jump
```
14:00:49 Event A
14:01:10 Event B (21 second gap)

Meaning: Either 1) Long operation, 2) System hang, or 3) Logs missing
Action: Check what happened during that 21 seconds
```

---

## If Stuck, Ask Yourself

1. **Do I understand what the user was trying to do?**
   - If no â†’ Review logs before first operation

2. **Can I trace the successful path?**
   - If no â†’ Find where it diverged

3. **Is the first error the root cause or a consequence?**
   - If consequence â†’ Keep looking backwards

4. **What safeguard should have prevented this?**
   - If no safeguard â†’ That's the root cause

5. **Would fixing this prevent recurrence?**
   - If no â†’ You haven't found root cause yet

6. **Can I prove this with log evidence?**
   - If no â†’ Don't make that claim

---

## Common Analysis Patterns

### Pattern 1: Shutdown During Active Request
```
Log: "Server shutting down"
Log: "Request still processing"
Log: "Connection closed"
Log: "Response lost"

Root Cause: No request-in-flight protection
Fix: Add logic to wait for active requests before shutdown
```

### Pattern 2: Resource Exhaustion
```
Log: "Connection pool exhausted"
Log: "New requests timing out"
Log: "Service unresponsive"

Root Cause: Too many concurrent requests for available resources
Fix: Increase pool size or implement backpressure/queuing
```

### Pattern 3: Cascading Errors
```
Log: "First error occurred"
Log: "Error handler tried to ..."
Log: "Error handler failed"
Log: "Cleanup failed"
Log: "System unstable"

Root Cause: First error, but poor error handling made it worse
Fix: Fix first error AND improve error handlers
```

### Pattern 4: Missing Dependency
```
Log: "Trying to use X"
Log: "X is null/undefined"
Log: "System crashed"

Root Cause: X wasn't initialized
Fix: Add initialization check or dependency verification
```

---

## When to Report Limitations

If you find that:
- Logs are incomplete/truncated â†’ State this clearly
- Key metrics missing â†’ State what's missing
- Timing data unavailable â†’ State you can't verify timing
- Cause unclear despite analysis â†’ State your uncertainty and what would clarify

**Example**:
> "The logs show the WebSocket closed at 14:01:10, but don't show why. Based on the file watcher message 7ms earlier, I infer the file change triggered shutdown, but the exact shutdown logic isn't captured in these logs. To confirm: 1) Check file modification timestamps, 2) Review shutdown handler code, 3) Add more detailed logging."

---

## Final Checklist Before Submitting Report

- [ ] Have I answered all 8 key questions?
- [ ] Is every claim supported by log evidence?
- [ ] Have I traced from user action to failure?
- [ ] Have I identified the ROOT cause (not symptoms)?
- [ ] Have I created expected vs actual comparison?
- [ ] Are recommendations prioritized by impact?
- [ ] Have I proposed testable verification?
- [ ] Is the language clear and non-technical where possible?
- [ ] Have I avoided speculation?
- [ ] Would someone unfamiliar with the system understand this?

If any are NO â†’ Keep revising.

---

## You're Done When

You can show:
1. âœ… Exact sequence of what happened (with timestamps)
2. âœ… Why it happened (with log evidence)
3. âœ… What should have happened (expected path)
4. âœ… Where it diverged (failure point)
5. âœ… Why it diverged (root cause)
6. âœ… Impact on the user (observable consequences)
7. âœ… How to fix it (3+ prioritized recommendations)
8. âœ… How to verify the fix (test scenarios)

All backed by explicit quotes from the logs.

---

**Good luck! Remember: Be systematic, question everything, and prove it with logs.**
